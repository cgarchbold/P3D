{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d0/grad/cgar/miniconda3/envs/dp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "from datasets import get_dataset\n",
    "from tqdm import tqdm\n",
    "from models import get_model\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "def get_config(config):\n",
    "    with open(config, 'r') as stream:\n",
    "        return yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/amo-d0/grad/cgar/miniconda3/envs/dp/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/u/amo-d0/grad/cgar/miniconda3/envs/dp/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/u/amo-d0/grad/cgar/miniconda3/envs/dp/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "  0%|          | 0/8153 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.])\n",
      "This is an image of a protest. A protester is holding a visual sign. There are roughly more than 20 people in the scene. There are roughly more than 100 people in the scene. The scene is at night. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = get_config('./configs/ucla_gan256.yaml')\n",
    "config['batch_size'] = 1\n",
    "# all we are really getting here is the path for the test data ^^^\n",
    "\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex', normalize=True).cuda()\n",
    "fid = FrechetInceptionDistance(feature=2048, normalize=True).cuda()\n",
    "inception = InceptionScore(normalize=True).cuda()\n",
    "\n",
    "#load model.pt\n",
    "#Create model\n",
    "discriminator, generator = get_model(config)\n",
    "\n",
    "generator = torch.nn.DataParallel(generator, device_ids=[0])\n",
    "\n",
    "checkpoint = torch.load(os.path.join('./ckpts','UCLA WGANGP (full conditional)','model.pt'))\n",
    "generator.load_state_dict(checkpoint['g_model_state_dict'])\n",
    "\n",
    "train_loader, test_loader = get_dataset(config)\n",
    "\n",
    "save_folder = os.path.join('./ckpts','UCLA WGANGP (full conditional)','test_set')\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "for sample in tqdm(test_loader):\n",
    "    fname = sample['fname'][0]\n",
    "    labels = Variable(sample['label']).cuda().type(torch.FloatTensor)[0]\n",
    "    image = Variable(sample['image']).type(torch.FloatTensor).cuda()\n",
    "    print(labels)\n",
    "    prompt = \"\"\n",
    "    negative_prompt=\"\"\n",
    "    if labels[0] == 1:\n",
    "        prompt+= \"This is an image of a protest. \"\n",
    "    else:\n",
    "        prompt+= \"This is not an image of a protest. \"\n",
    "    if labels[1] == 1:\n",
    "        prompt+= \"The protest is violent. \"\n",
    "    if labels[2] == 1:\n",
    "        prompt+= \"A protester is holding a visual sign. \"\n",
    "    if labels[3] == 1:\n",
    "        prompt+= \"The sign contains a photo of a person. \"\n",
    "    if labels[4] == 1:\n",
    "        prompt+= \"There is fire or smoke in the scene. \"\n",
    "    if labels[5] == 1:\n",
    "        prompt+= \"Police or troops are present in the scene. \"\n",
    "    if labels[6] == 1:\n",
    "        prompt+= \"There are children in the scene. \"\n",
    "    if labels[7] == 1:\n",
    "        prompt+= \"There are roughly more than 20 people in the scene. \"\n",
    "    if labels[8] == 1:\n",
    "        prompt+= \"There are roughly more than 100 people in the scene. \"\n",
    "    if labels[9] == 1:\n",
    "        prompt+= \"There are flags in the scene. \"\n",
    "    if labels[10] == 1:\n",
    "        prompt+= \"The scene is at night. \"\n",
    "    if labels[11] == 1:\n",
    "        prompt+= \"There are one or more people shouting. \"\n",
    "    print(prompt)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
